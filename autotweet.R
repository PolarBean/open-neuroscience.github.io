library(tidyverse)
library(googlesheets4)

# helper functions -----
create_yaml <- function(title, authors, categories){
  yaml_break <-"---"
  tit <- paste("title:", shQuote(title))
  dat <- paste("date:", Sys.Date())
  authors <- stringr::str_c(shQuote(authors), collapse=" , ")
  authors <- paste0("[", authors,"]")
  aut <- paste("authors:", authors)
  lay <- "layout: post"
  # categories come shQuoted properly from parse_tags()
  categories <- paste0("[", categories,"]")
  catt <- paste("categories:", categories)
  # tags are the same as categories for now...
  tagg <- paste("tags:", categories)

  # make the yaml
  output <- paste(yaml_break,
                  tit,
                  dat,
                  aut,
                  lay,
                  catt,
                  tagg,
                  yaml_break, sep = "\n")
  return(output)

}

create_body <- function(title, image, description, authors, website, video, post_author){
  # remove all spaces in title
  title <- gsub(x = title , pattern = " ", replacement = "_")
  # remove all other punctuation things
  title <- stringr::str_replace_all(string = title,
                                    pattern="[[:punct:]]",
                                    replacement="_")
  # make folder on posts if it doesn't exist
  root = "content/en/post"
  if(title %in% list.files(root) == FALSE){
    # make dir
    dir.create(file.path(root, title))
  }
  # download image and return the new path
  image <- get_image(image_link = image, path = file.path(root, title))
  # add the "![]()"
  # if the name is called "featured" we don't need to link to it in the post
  #image <- paste0("![](", image, ")")

  # bind things into "description"
  if (is.na(video)){
    description <- paste(description,
                         "## Project Author(s)",
                         authors,
                         "## Project Links",
                         website,
                         "***",
                         "This post was automatically generated by",
                         post_author,
                         sep="\n")
  } else {

    # short cut is {{< youtube w7Ft2ymGmfc >}}
    # emb_video <- paste("{{< youtube")

    description <- paste(description,
                         "## Project Author(s)",
                         authors,
                         "## Project Links",
                         website,
                         "## Project Video",
                         video,
                         "***",
                         "This post was automatically generated by",
                         post_author,
                         sep="\n")
  }

  # paste things
  output <- paste(
    # image,
    description,
    '***',
    sep="\n")
  return(output)
  }

get_image <- function(image_link, path){

  # we will default to our logo when things are nasty on the image side
  our_logo <- "https://raw.githubusercontent.com/open-neuroscience/open-neuroscience-website/master/content/en/authors/admin/avatar.png"

  url <- case_when(str_detect(image_link, "png") ~ str_extract(image_link, ".+png"),
                   str_detect(image_link, "jpg") ~ str_extract(image_link, ".+jpg"),
                   str_detect(image_link, "gif") ~ str_extract(image_link, ".+gif"),
                   # When it doesn't detect, we will fall back to the logo image
                   TRUE ~ our_logo
                   )

  filename <- case_when(
    str_detect(image_link, "png") ~ file.path(path, "featured.png"),
    str_detect(image_link, "jpg") ~ file.path(path, "featured.jpg"),
    str_detect(image_link, "gif") ~ file.path(path, "featured.gif"),
    # we need to fail with featured.png because our logo image is .png
    TRUE ~ file.path(path, "featured.png")
  )

  # check whether there's a problem with the image
  if(url == our_logo){
    write.table(paste("problem with", image_link),
                file = paste0(tools::file_path_sans_ext(filename), ".txt"),
                row.names = FALSE)
  }

  # if this thing is not an URL it will fail
  url <- download.file(url, filename, mode = "wb")

  # check if the image can be loaded
  img <- try(imager::load.image(filename))
  if (class(img) == "try-error"){
    write.table(paste("problem with", image_link),
                file = paste0(tools::file_path_sans_ext(filename), ".txt"),
                row.names = FALSE)
    # move the logo into featured
    file.copy("content/en/authors/admin/avatar.png",
              filename,
              overwrite = TRUE)

  }

  return(filename)
}

write_md <- function(filename, content){
  fileConn <- file(filename)
  writeLines(content, fileConn)
  close(fileConn)
}

parse_tags <- function(df){
  df %>%
    select(starts_with("Project categories")) %>%
    group_by(row=row_number()) %>%
    pivot_longer(cols = starts_with("Project")) %>%
    filter(value != "NA") %>%
    summarise(categories = paste(shQuote(value), collapse=",")) %>%
    pull(categories)

}

parse_hashtags <- function(df){
  # this is basically the same as parse_tags()
  # but it gives them back as a string of hashtags
  df %>%
    select(starts_with("Project categories")) %>%
    group_by(row=row_number()) %>%
    pivot_longer(cols = starts_with("Project")) %>%
    filter(value != "NA") %>%
    # remove all white spaces and turn CamelCase
    mutate(value = snakecase::to_upper_camel_case(value)) %>%
    summarise(categories = paste0("#", value, collapse=" ")) %>%
    pull(categories)

}


# this is the ID
ID <- "1qF5P8RKBSiE6qyInIoTBHdq2m9o5ZnvhnGKkmaJ83uI"
gs4_deauth()
target <- read_sheet(ID)

# this comes handy for later,
# so that we don't add a bunch of columns when we write back
original_columns <- names(target)

# parse tags
target$hashtags <- parse_hashtags(target)

ON_link <- function(title){
  p <- "https://open-neuroscience.com/en/post/"
  title <- gsub(x = tolower(title) , pattern = " ", replacement = "_")
  # remove all other punctuation things
  title <- stringr::str_replace_all(string = title,
                                    pattern="[[:punct:]]",
                                    replacement="_")
  return(paste0(p, title))
}

#
tweet_maker <-
  select(target,
         `Project Title`,
         `Project Author`,
         `Post Author Twitter handle`,
         `Link to Project Website or GitHub repository`,
         `Description of the project`,
         tags) %>%
  mutate(by = "Created by:",
         find_more = "Find More at:",
         thread = "Thread below -->",
         handle = `Post Author Twitter handle`,
         # we turn them to "" so we can paste nothing if they don't have @
         handle = ifelse(str_detect(handle, "^@"), handle, NA),
         handle = ifelse(is.na(handle), "", handle),
         initial_tweet = paste(`Project Title`,
                            by,
                            `Project Author`,
                            handle,
                            find_more,
                            #`Link to Project Website or GitHub repository`,
                            ON_link(`Project Title`),
                            thread,
        # separate with carriage return
                            sep="\n")
  )


# create sentences from tweets
sentences <- tokenizers::tokenize_sentences(tweet_maker$`Description of the project`)


extra_chars <- " ... ðŸ§µ"
chunk_nchar <- 260
on_blurb <-
  function(hashtags){
    glue::glue(
      "For this and other projects related to {hashtags} visit https://openneuroscience.com"
    )
  }

# we can check the lenghts here...
sentence_stats <- map(sentences,
                         function(tt) data.frame(
                           descr_len = length(tt),
                           n = nchar(tt),
                           sentence = tt)
                         )
# set the names for binding
# names(sentence_stats) <- tweet_maker$`Project Title`

sentence_stats <-
sentence_stats %>%
  bind_rows(.id="id") %>%
  group_by(id) %>%
  mutate(cum_chars = cumsum(n),
         # we create an id using modulo
         # we add one to be able to have a thread_id = 0
         thread_id = floor(cum_chars / chunk_nchar) + 1,
         last_tweet = last(thread_id),
         )

# We are using numerical IDs in case projects are duplicated
# it's often difficult to find dupes if people don't write
# consistently (e.g., DeepLabCut vs Deep lab cut: a tool for...)
id_key <- tweet_maker %>%
  mutate(id = row_number(),
         id = as.character(id)) %>%
  select(id, `Project Title`, handle, link = `Link to Project Website or GitHub repository`)

open_sentences <-
  mutate(id_key,
         sentence = glue::glue(
           "{`Project Title`} {link} {extra_chars}"
         ),
         sentence = as.character(sentence),
         thread_id = 0)

closing_sentences <-
  sentence_stats %>%
  summarise(id = unique(id),
            last_tweet = last(thread_id) + 1) %>%
  mutate(sentence = map_chr(target$hashtags, on_blurb))

# Add the closing sentence
bind_rows(sentence_stats, open_sentences) %>%
  bind_rows(closing_sentences) %>%
  arrange(id, thread_id) %>%
  mutate(lag_id = lag(thread_id),
         thread_id = if_else(is.na(thread_id), lag_id + 1, thread_id)) %>%
  group_by(id, thread_id) %>%
  summarise(sentence = paste(sentence, collapse = " ")) %>% View()
